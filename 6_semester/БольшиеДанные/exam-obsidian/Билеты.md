

# 1. Концепция Big Data. Предпосылки появления Big Data. Особенности задач Big Dataв различных областях. Характеристики Big Data. BA (Business Analytics) – бизнесаналитика, Data Mining, KDD, ML-модели (Machine Learning - модели машинного обучения).

Big Data - это термин, который относится к огромным, сложным и быстрорастущим наборам данных, которые традиционные методы обработки данных не в состоянии эффективно анализировать. 

### Характеристики Big Data:

- Объем (Volume):  Большие объемы данных, которые традиционные методы обработки не могут эффективно анализировать. Объем данных должен превышать 150 Гб в сутки
    
- Скорость (Velocity):  Данные поступают  с высокой скоростью, и их нужно  обрабатывать в реальном времени.
    
- Разнообразие (Variety):  Big Data включает различные типы данных: структурированные, полуструктурированные и неструктурированные.
    
- Достоверность (Veracity):  Качество данных  играет  важную роль, так как  неверные  данные  могут  привести к  неверным  результатам анализа.
    
- Ценность (Value):  Big Data ценна, потому что  она  позволяет  получить  более полную картину  ситуации,  выявлять  скрытые  закономерности  и  принимать  более  информированные решения.

### Предпосылки Появления Big Data:  
- Технологический прогресс:
	- Увеличение вычислительной мощности, что позволило обрабатывать огромные объемы информации.
	- Развитие интернет привело к  взрывному росту  количества генерируемых данных.
	- Развитие алгоритмов и инструментов анализа данных:  Новые алгоритмы  и инструменты, такие как Hadoop, Spark и NoSQL,  позволили эффективно анализировать большие объемы данных.

- Изменения в поведении пользователей:
	- Активное использование интернета:  Люди стали активно использовать интернет для общения, покупок, развлечений, что породило огромный поток данных.
	- Создание цифрового следа:  Каждая онлайн-активность пользователя  оставляет цифровой след, который может быть использован для анализа поведения.
	- Рост количества данных в онлайн-пространстве:  Социальные сети, онлайн-магазины,  видеохостинги генерируют  огромные объемы данных, доступные для анализа.
- Рост потребности в анализе данных:
	- Поиск новых возможностей:  Анализ больших объемов данных помогает выявить  скрытые закономерности и тенденции,  что  позволяет  компаниям и организациям  принимать  более обоснованные  решения.
	- Оптимизация процессов:  Анализ  Big Data  помогает оптимизировать бизнес-процессы,  улучшить  сервис  и повысить  эффективность работы.
	- Принятие решений на основе данных:  Анализ  Big Data  предоставляет  более полную картину  ситуации,  что  помогает  принимать  более  информированные решения.

### Особенности задач Big Data в различных областях:  
• Бизнес:

   * Маркетинг:  Анализ покупательского поведения, таргетированная реклама, персонализация предложения.
   * Аналитика продаж:  Прогнозирование спроса,  определение  популярных товаров,  оптимизация  ценообразования.

   * Оптимизация цепочек поставок:  Снижение  запасов,  улучшение  логистики,  оптимизация  планирования.

  

• Здравоохранение:

   * Диагностика болезней:  Разработка  новых  методов  диагностики,  ранняя  диагностика  болезней.

   * Персонализированная медицина:  Разработка  индивидуальных  планов  лечения,  улучшение  результатов  лечения.

   * Оптимизация  медицинских процессов:  Повышение  эффективности  медицинских  учреждений,  снижение  затрат.

  

• Финансы:

   * Обнаружение мошенничества:  Предупреждение  и  пресечение  финансовых  преступлений.

   * Управление рисками:  Определение  и  управление  рисками  в  инвестировании.

   * Прогнозирование цен на акции:  Принятие  инвестиционных  решений  на  основе  анализ  Big Data.

  

• Образование:

   * Персонализация обучения:  Разработка  индивидуальных  планов  обучения  для  каждого  студента.

   * Оценка эффективности обучения:  Анализ  эффективности  методов  обучения,  определение  неэффективных  практик.

   * Анализ  результатов:  Изучение  влияния  различных  факторов  на  успеваемость  студентов.

  

• Научные исследования:

   * Анализ  больших  наборов  данных:  Поиск  скрытых  закономерностей  и  тенденций  в  научных  данных.

   * Разработка  новых  моделей:  Создание  новых  теорий  и  моделей  на  основе  анализа  Big Data.

   * Оптимизация  экспериментов:  Разработка  более  эффективных  методов  проведения  научных  исследований.

  

Бизнес-Аналитика (BA) - Область, которая  занимается  использованием  данных  для  решения  бизнес-задач.

  

Data Mining -  Процесс  извлечения  знаний из  больших  наборов  данных.

  

KDD (Knowledge Discovery in Databases) -  Процесс  поиска  полезных  знаний  в больших  наборах  данных.  Включает  в  себя  этапы  подготовки  данных,  выбора  моделей,  обучения,  оценки  и  визуализации  результатов.

  

ML-Модели (Machine Learning - модели машинного обучения) -  Используются  для  анализ  Big Data  и  получения  прогнозов.  Примеры  моделей  машинного  обучения:  регрессия,  классификация,  кластеризация.

2. # Жизненный цикл бизнес-аналитики Больших Данных. Подготовка и очистка данных. Конструирование признаков. Разбиение на множества. Построение модели. Оценка качества. Эксплуатация модели. Инструменты для анализа больших данных.
    

Жизненный цикл бизнес-аналитики Больших Данных (Big Data) - это структурированный процесс, который включает в себя ряд шагов, необходимых для эффективного анализа больших объемов данных и получения ценных знаний. Он включает в себя следующие этапы:

  

1. Формулировка задачи: 

• Определение бизнес-целей:  Что нужно достичь с помощью анализа данных? 

• Четкое описание задачи:  Какую информацию нужно получить из данных? 

• Определение ключевых показателей (KPI):  Как будет измеряться успех анализа? 

2. Подготовка и очистка данных:

• Сбор данных:  Сбор данных из различных источников (базы данных,  файлы,  лог-файлы,  социальные сети,  IoT-устройства).

• Преобразование данных:  Преобразование данных в  единый формат для дальнейшего анализа (например,  объединение  таблиц,  изменение  типов  данных).

• Очистка данных:  Удаление  некорректных,  дублирующихся  и  пропущенных  данных  (например,  замена  пропущенных  значений,  исправление  ошибок).

3. Конструирование признаков (Feature Engineering):

• Выбор  релевантных признаков:  Выбор  характеристик  данных,  которые  могут  влиять  на  целевой  показатель  (например,  возраст,  пол,  доход,  местоположение  клиента).

• Создание новых признаков:  Создание  новых  характеристик  на  основе  существующих  данных  (например,  создание  комбинаций  существующих  признаков).

• Преобразование признаков:  Преобразование  существующих  признаков  для  улучшения  их  качества  (например,  стандартизация  данных).

4. Разбиение на множества (Data Splitting):

• Разделение  данных  на  обучающее,  проверочное  и  тестовое  множества. 

• Обучающее множество:  Используется  для  обучения  модели  машинного  обучения.

• Проверочное множество:  Используется  для  оптимизации  параметров  модели.

• Тестовое множество:  Используется  для  оценки  качества  обученной  модели.

5. Построение модели:

• Выбор  модели  машинного  обучения:  Выбор  модели,  которая  подходит  для  решения  задачи  (например,  регрессия,  классификация,  кластеризация).

• Обучение  модели:  Обучение  модели  на  обучающем  множестве  данных.

• Оптимизация  параметров  модели:  Настройка  параметров  модели  на  проверочном  множестве  данных.

6. Оценка качества модели:

• Оценка  качества  модели  на  тестовом  множестве  данных  с  помощью  соответствующих  метрических  показателей  (например,  точность,  полнота,  F1-мера).

• Сравнение  различных  моделей:  Выбор  модели  с  наилучшими  показателями  качества.

7. Эксплуатация модели:

• Развертывание  модели:  Размещение  обученной  модели  в  производственную  среду.

• Мониторинг  модели:  Регулярный  мониторинг  работы  модели  и  оценка  ее  эффективности.

• Обновление  модели:  Переобучение  модели  на  новых  данных  для  поддержания  ее  актуальности.

Инструменты для анализа больших данных:

• Hadoop:  Платформа  для  распределенного  хранения  и  обработки  данных.

• Spark:  Платформа  для  быстрой  обработки  данных  в  реальном  времени.

• NoSQL:  Базы  данных,  которые  не  требуют  строгой  структуризации  данных.

• Cloud Computing:  Облачные  вычисления  позволяют  эффективно  хранить  и  обрабатывать  большие  объемы  данных.

• Python:  Язык  программирования,  широко  используемый  для  анализа  данных.

• R:  Язык  программирования,  используемый  для  статистического  анализа  данных.

• Tableau, Power BI, Qlik Sense:  Инструменты  для  визуализации  данных.

• Machine Learning Libraries:  Библиотеки  для  машинного  обучения  (например,  scikit-learn,  TensorFlow,  PyTorch).

3. # Требования к данным в проектах бизнес-аналитики. Формы представления данных. Типы шкал. Информативность данных. Представление наборов данных. Особенности больших данных.
    

Требования к данным в проектах бизнес-аналитики:

• Точность: Данные должны быть точными и соответствовать действительности. 

• Полнота: Данные должны быть полными, без пропусков и неточностей.

• Согласованность: Данные должны быть согласованы между собой, не должно быть противоречий.

• Актуальность: Данные должны быть актуальными и отражать текущую ситуацию.

• Релевантность: Данные должны быть релевантными для решения поставленной бизнес-задачи.

  

Формы представления данных:

• Структурированные данные:  Данные, которые организованы в таблицы с определенными столбцами и строками. Например, базы данных, таблицы Excel.

• Полуструктурированные данные: Данные, которые имеют некоторую структуру, но не являются строго табличными. Например, JSON, XML,  логи-файлы.

• Неструктурированные данные: Данные, которые не имеют определенной структуры. Например, текст, изображения, видео, аудио.

  

Шкала измерения - это система категоризации и количественной оценки данных, используемая в аналитике.  Она определяет, как переменные измеряются и каким образом можно проводить операции с полученными данными. Существует четыре основных типа шкал измерения:

  

1. Номинальная шкала:

• Описание:  Данные классифицируются в категории без естественного порядка или иерархии. 

• Примеры: Пол (мужской, женский), цвет глаз (голубой, зеленый, карие), тип автомобиля (седан, внедорожник, пикап).

• Операции: Только подсчет частоты и сравнение. 

  
  
  

2. Порядковая шкала:

• Описание: Данные классифицируются в категории с естественным порядком или иерархией. Однако расстояния между категориями не равны.

• Примеры: Уровень образования (начальное, среднее, высшее), уровень согласия (полностью согласен, согласен, не согласен, полностью не согласен),  ранжирование товаров по предпочтениям.

• Операции: Подсчет частоты, сравнение,  ранжирование.

  

3. Интервальная шкала:

• Описание: Данные классифицируются в категории с равными интервалами между ними. Ноль является произвольным значением, а не отсутствием величины.

• Примеры: Температура по Цельсию или Фаренгейту, IQ-тест, даты.

• Операции: Подсчет частоты, сравнение, ранжирование, сложение, вычитание, вычисление среднего значения.

  

4. Относительная шкала:

• Описание: Данные классифицируются в категории с равными интервалами, а также с абсолютным нулем. 

• Примеры:  Вес, рост, доход, время,  количество товаров.

• Операции: Все операции интервальной шкалы +  деление, умножение, вычисление отношения.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdevJqkFm4XrMrgqoojG8EYyPi8KXsv0rIiFF7V55IN7M0H-7hRTnENJz2Oohho3lARtYJ3Jg28cRXkdAGBVog8aaoKXaPi82IEXrV3wnidys__LfXEXAi-6t6nsxvEaB5Tmztn3yqTtdXpjnMx8q6ozxtcBZtGk12bwuc0?key=DNFeKhMTGi0oG-o6qf005Q)

  Информативность данных:

• Смысловая нагрузка: Данные должны иметь четкую смысловую нагрузку, быть понятными и интерпретируемыми.

• Релевантность к бизнес-задаче: Данные должны быть релевантны к решаемой бизнес-задаче, способствовать достижению поставленных целей.

• Доступность: Данные должны быть доступны для анализа и обработки.

  

Среди неинформативных признаков выделяется четыре типа:

1. признаки, содержащие только одно значение (рис. а);
    
2. признаки, содержащие в основном одно значение (рис.6);
    
3. признаки с уникальными значениями (рис. в);
    
4. признаки, между которыми имеет место сильная корреляция, — в этом случае для анализа можно взять один столбец (рис. г).
    

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXet6Z7S8uS4s4sXhHPU69WJVgVz38Q5vesiDtRPYb4wPADQ4CGCvGsVtkOPCwgHl1yIYcNjHjhmKC5h2K-Vz3pn7PrVW5aNX8QdR9HN6zUAvvJQ4r4HXalKu2tw7myl965tGpSow1sqscGi5sOaBCwbVuOkxKvvVfxSRAxixA?key=DNFeKhMTGi0oG-o6qf005Q)

  

Представление наборов данных:

• Таблицы:  Данные  представлены  в  виде  таблиц  с  заголовками  столбцов  и  строк.

• Графики:  Визуальное  представление  данных  в  виде  графиков,  гистограмм,  диаграмм  и  т.д.

• Текстовые  файлы:  Данные  представлены  в  виде  текстовых  файлов  с  разделителями.

• Базы  данных:  Данные  хранятся  в  структурированных  базах  данных.

  

Особенности больших данных (Big Data):

• Объем: Большие объемы данных, которые традиционные методы обработки не могут эффективно анализировать.

• Скорость:  Данные поступают  с высокой скоростью,  и  их  нужно  обрабатывать  в  реальном  времени.

• Разнообразие:  Big Data включает различные типы данных: структурированные, полуструктурированные и неструктурированные.

• Достоверность:  Качество  данных  играет  важную роль, так как  неверные  данные  могут  привести к  неверным  результатам анализа.

4. # Подготовка данных к анализу. Особенности данных. Формализация данных. Методы сбора данных. Методы предподготовки данных. Роль визуализации в аналитике Больших Данных.
    

Особенности данных:

• Тип данных:  Структурированные (таблицы, базы данных), полуструктурированные (XML, JSON), неструктурированные (текст, изображения, видео).

• Источник данных:  Внутренние базы данных, внешние источники (API, веб-сайты),  социальные сети,  IoT-устройства.

• Качество данных:  Точность, полнота, согласованность, актуальность, релевантность.

• Объем данных:  Размер набора данных, который может варьироваться от небольших до огромных.

  

Формализация данных - это процесс преобразования данных из неструктурированного или полуструктурированного формата в формат данных, который может быть использован компьютерами для анализа, обработки, хранения, передачи или использования в других целях.

  

Формализация данных включает в себя:

• Преобразование данных:  Преобразование данных в  единый формат для  дальнейшего  анализа (например,  объединение  таблиц,  изменение  типов  данных,  стандартизация  форматов  даты  и  времени).

• Создание  новых  признаков:  Создание  новых  характеристик  на  основе  существующих  данных  (например,  создание  комбинаций  существующих  признаков).

• Разбиение на множества:  Разделение  данных  на  обучающее,  проверочное  и  тестовое  множества  для  обучения  и  оценки  моделей.

  

Методы сбора данных:

• Внутренние источники данных:  Сбор данных из  внутренних  баз  данных,  файлов  и  лог-файлов.

• Внешние источники данных:  Сбор  данных  с  веб-сайтов,  API,  социальных  сетей  и  других  внешних  источников.

• Скрапинг:  Автоматический  сбор  данных  с  веб-сайтов  с  помощью  специальных  инструментов.

• API:  Использование  интерфейсов  программного  обеспечения  для  получения  данных  от  других  систем.

  
  

Методы предподготовки данных:

• Очистка данных:  Удаление  некорректных,  дублирующихся  и  пропущенных  данных  (например,  замена  пропущенных  значений,  исправление  ошибок).

• Обработка  пропущенных  данных:  Замена  пропущенных  значений  с  помощью  различных  методов  (например,  замена  на  среднее  значение,  введение  специального  значения).

• Обработка  выбросов:  Удаление  или  коррекция  выбросов  (нетипичных  значений),  которые  могут  искажать  результаты  анализа.

• Трансформация данных:  Преобразование  данных  для  улучшения  их  качества  (например,  стандартизация  данных,  логарифмирование).

  

Роль визуализации в аналитике Больших Данных:

• Понимание данных:  Визуализация  помогает  лучше  понять  структуру  и  закономерности  данных.

• Обнаружение  аномалий:  Визуализация  позволяет  легко  обнаружить  выбросы  и  аномалии.

• Коммуникация  результатов:  Визуализация  позволяет  представить  результаты  анализа  в  доступной  и  понятной  форме.

• Интерактивность:  Интерактивные  визуализации  позволяют  изучать  данные  в  более  подробном  виде  и  получать  более  глубокое  понимание.

  

5. # Консолидация данных. Особенности бизнес-данных, накопленных в компаниях. Методы сбора и загрузки данных из различных источников. Обогащение данных.
    

Консолидация — это процесс объединения данных из различных источников в единую, согласованную структуру. Это позволяет получить комплексное представление о бизнес-процессах, клиентах, продуктах и т.д.

  

Основные задачи консолидации данных:

1. выбор источников
    
2. разработка стратегии консолидации
    
3. оценка качества данных
    
4. обогащение
    
5. очистка
    
6. перенос в хранилище данных.
    

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc2fyXt0seIp4XbpxfM0hq1wgSYmxTH6nvVdu-KiV6Ssqdd89CAM81BltaTWovzcGn6Pdw1mw_XVCPPd3rYo14lwEQdxVgHSJxq27p5nmckDxW-SB8g37lTFOHXcVXX511njA-A1G9bJK6YAifY8346yMdRDktBkJV18MrKiw?key=DNFeKhMTGi0oG-o6qf005Q)

Особенности бизнес-данных, накопленных в компаниях:

• Разнообразие источников: Данные хранятся в различных системах (CRM, ERP, веб-аналитика, базы данных, логи, отчеты).

• Разные форматы: Данные могут быть в разных форматах (текстовые файлы, таблицы, базы данных, JSON, XML).

• Несогласованность:  Разные системы могут использовать разные термины, единицы измерения, структуры данных.

• Качество: Данные могут быть неполными, неточными, дублированными.

• Объем: Объем данных постоянно растет.

  
  

Методы сбора и загрузки данных из различных источников

1. Получение из учетных систем. Обычно в учетных системах есть различные механизмы построения отчетов и экспорта данных, поэтому извлечение нужной информации из них чаще всего относительно несложная операция.

2. Получение данных из косвенных источников информации. О многих показателях можно судить по косвенным признакам, и этим нужно воспользоваться. 

3. Использование открытых источников. Большое количество данных присутствует в таких открытых источниках, как статистические сборники, отчеты корпораций, опубликованные результаты маркетинговых исследований и пр.

4. Приобретение аналитических отчетов у специализированных компаний. На рынке работает множество компаний, которые профессионально занимаются сбором данных и представлением результатов клиентам для последующего анализа. Собираемая информация обычно представляется в виде различных таблиц и сводок, которые с успехом можно применять при анализе. Стоимость получения подобной информации чаще всего относительно невысока.

5. Проведение собственных маркетинговых исследований и аналогичных мероприятий по сбору данных. Этот вариант сбора данных может быть достаточно дорогостоящим, но в любом случае он существует.

6. Ввод данных вручную. Данные вводятся по различного рода экспертным оценкам сотрудниками организации. Такой метод является наиболее трудоемким.

  

Обогащение — процесс дополнения данных некоторой информацией, позволяющей повысить эффективность решения аналитических задач. Обогащение позволяет более эффективно использовать консолидированные данные. Его необходимо применять в тех случаях, когда данные содержат недостаточно информации для удовлетворительного решения определенной задачи анализа. Обогащение данных позволяет повысить их информационную насыщенность и, как следствие, значимость для решения аналитической задачи.

  
  
  

6. # Трансформация данных. Группировка, свертка столбцов (транспонирование таблицы), фильтры строк и столбцов, слияние, квантование, нормализация и кодирование.
    

Трансформация данных - это ключевой этап в аналитике данных, который позволяет преобразовать сырые данные в более удобный и информативный формат для анализа. Существует множество методов трансформации, каждый из которых решает свои задачи.

  

Группировка:

• Цель: Объединение данных с одинаковыми значениями в группы для проведения агрегирующих операций, таких как суммирование, усреднение, подсчет. 

• Пример:  Сводная таблица с продажами товаров по категориям.

• Преимущества: Упрощение анализа, выявление трендов, сокращение объема данных.

• Недостатки: Потеря детализации, возможные сложности с реализацией сложных условий группировки.

  

 Свертка столбцов (транспонирование таблицы):

• Цель:  Перестановка строк и столбцов для изменения представления данных.

• Пример:  Преобразование таблицы с продажами по месяцам в таблицу с продажами по продуктам.

• Преимущества: Изменение структуры данных для анализа и визуализации.

• Недостатки:  Может потребовать дополнительной обработки для правильного отображения.

  

Фильтры строк и столбцов:

• Цель: Выборка данных, удовлетворяющих заданным критериям. 

• Пример: Выбор клиентов с определенным статусом, отбор данных по конкретному периоду времени.

• Преимущества: Уменьшение объема данных для анализа, фокусировка на важной информации.

• Недостатки:  Потеря важных данных при неправильном выборе критериев.

  
  

Слияние:

• Цель: Объединение данных из разных таблиц по общему ключу.

• Пример:  Соединение данных о клиентах с данными о их заказах.

• Преимущества:  Создание единых наборов данных из разных источников.

• Недостатки:  Сложность реализации при несовпадении ключей.

  

Квантование:

• Цель: Преобразование непрерывных данных в дискретные интервалы.

• Пример:  Разбиение данных о зарплатах на категории: низкие, средние, высокие.

• Преимущества:  Упрощение анализа данных, визуализация.

• Недостатки:  Потеря точности данных.

  

Нормализация:

• Цель: Приведение данных к единому диапазону значений для улучшения сравнения данных из разных источников.

• Пример:  Нормализация значений цены продуктов для сравнения товаров из разных категорий.

• Преимущества:  Улучшение интерпретации данных, возможность использования алгоритмов, требующих определенного диапазона значений.

• Недостатки:  Потеря информации о дисперсии данных.

  

Кодирование:

• Цель: Преобразование категориальных данных в числовой формат для использования в алгоритмах машинного обучения.

• Пример:  Преобразование названий городов в числовые коды.

• Преимущества:  Использование алгоритмов машинного обучения для категориальных данных.

• Недостатки:  Потеря информации о категориальных значениях.

  
  
  
  

7. # Визуализация данных. Визуализаторы общего назначения и специализированные. OLAP-анализ. Визуализаторы оценки качества моделирования. Визуализаторы, интерпретации результатов анализа. Визуальный управленческий контроль.
    

Визуализация данных - это мощный инструмент, позволяющий преобразовать сырые данные в легко усваиваемые визуальные представления. Это помогает людям лучше понять закономерности, тренды и выводы, полученные в результате анализа данных. 

  

Визуализаторы общего назначения - применяются для решения типовых задач, визуальной оценки качества данных. Не связаны с каким-либо определенным видом задач анализа или типом данных и могут использоваться на любом этапе аналитического прогресса. Это типовые визуализаторы: графики и диаграммы, графы, гистограммы и их разновидности, статистические характеристики и др. 

  

Специализированные визуализаторы - используются для ряда специфических задач. Например, карты Кохонена специально разработаны для визуализации результатов кластеризации, матрицы классификации используются в основном для проверки состоятельности классификационных моделей; диаграммы рассеивания помогают оценивать корректность работы регрессионных моделей. 

  

 OLAP (On-Line Analytical Processing) - это технология, которая позволяет анализировать данные с разных ракурсов, используя многомерные модели данных. 

OLAP-анализ - комплекс методов для визуализации многомерных данных. 

• Визуализация в OLAP:  Визуализаторы OLAP-анализа помогают  "срезать" многомерные данные под разными углами, создавая интерактивные сводные таблицы, диаграммы и графики, которые можно вращать, фильтровать и анализировать.

Пример в логинове - OLAP-куб!

  

Визуализаторы оценки качества моделирования: независимо от вида построенной модели, прежде чем применять ее на практике, необходимо оценить ее качество, то есть определить, насколько правильно и точно она решает поставленную задачу. 

Можно выделить две составляющие модели:

Адекватность - насколько точно модель описывает исследуемый объект или процесс;

Корректность - насколько правильно модель может работать со всеми возможными входными данными. 

Для исследования адекватности и корректности моделей в состав аналитических приложений включаются специальные визуализаторы, которые позволяют оценить точность модели и ее реакцию на те или иные входные данные. 

Набор визуализаторов для оценки качества моделей следующий:

- матрица классификации
    
- диаграмма рассеяния (визуализатор, который применяется для оценки качества моделей в случае непрерывной выходной переменной; это график, по одной оси которого откладывается целевые значения выходной переменной (то есть те, которые заданы в качестве эталона для обучения), а по другой - реальные значения, полученные на выходе модели, используется для задач регрессии). 
    
- ретропрогноз (используется для проверки качества прогностической модели). буквально означает “прогноз в прошлом”. Его сущность заключается в том, чтобы применить построенную модель к подмножеству данных из прошлого и оценить, насколько полученный прогноз соответствует реальным значениям ряда, имевшим место в прошлом. Направлен не в будущее, а формируется параллельно его прошлым значениям и сравнивается с ними. 
    

  

Визуализаторы интерпретации результатов анализа: двевовидные визуализаторы, визуализаторы связей, карты.

  

Древовидные визуализаторы - могут использоваться везде, где результаты аналитической обработки данных образуют иерархические уровни и могут быть разделены на предков и потомков. 

  

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdShGnFJmZYjLj-WNpARE6_J2QRWWFcFq4UeJxZ8KVIPtEgQgtHiB10knoOUZaT-3C5UscT-zNEodXjqludh5JISFEe1rtjebsiXUc-bHylMJ47pLA6s_om_8nqHIqCS8Aod-WArcJDriEhX0PHduFE-BqV95Tt25uDXR3f?key=DNFeKhMTGi0oG-o6qf005Q)

Визуализация связей - для анализа характера и степени взаимной зависимости между различными объектами. В визуализации связей объекты представляются в виде значков, а связи - в виде линий. Сила связи показывается толщиной/цветом линии. 

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcKugOx6GEPg5YbtjROrWrFuM769byFr0oY5VyW8-tA4C-dfj42dL_xnHLnTbxedDo6VLGAO2TbZq5W3kVumChTdFlTtBojXO64Oj7M5wNNNTMAI5bllm01MBDNXSXo6OM800cYYg2w6CbKZaqix28M1ZjNHks_nVeWPXsPsQ?key=DNFeKhMTGi0oG-o6qf005Q)

Карты - позволяют наглядно представить данные, связанные с географическим расположением исследуемых объектов и процессов. Это могут быть демографические данные и т. д. Реально просто карта географическая

  

Визуальный управленческий контроль:

• Цель: Предоставить руководителям  четкую и лаконичную картину ключевых показателей бизнеса,  чтобы они могли быстро оценить ситуацию и принять решения. 

• Визуальные элементы: Графики, диаграммы, индикаторы, карточки, таблицы с  динамическими обновлениями.

  

8. # Очистка данных. Оценка качества данных. Методы оценки качества данных. Фильтрация данных, обработка дубликатов и противоречий. Выявление аномальных значений. Сокращение размерности данных. Сокращение числа признаков и записей.
    

Очистка данных – это процесс удаления ошибок, неточностей, несоответствий и неполноты данных, чтобы сделать их более точными, надежными и пригодными для анализа.

  

Качество данных - совокупность их свойств и характеристик, определяющих степень пригодности для анализа.

  

Перед применением тех или иных методов очистки необходимо выполнить оценку качества данных (Assessment Data Quality — ADQ) с целью выявления наиболее характерных проблем и уровня их сложности, а также для выработки стратегии очистки. В результате такой оценки, возможно, будет принято решение отказаться от очистки некоторых данных, поскольку это слишком сложно, дорого или не приведет к желаемым результатам. 

  

Оценка качества данных может быть реализована тремя способами:

• Единовременная оценка качества проводится для некоторого массива данных (файла или таблицы) один раз, после чего, в зависимости от ее результатов, к данным применяются те или иные методы очистки, иногда же данные признаются качественными и остаются без изменений.

• Мониторинг: поток данных, поступающих из различных источников в ХД и далее в аналитическую систему, непрерывно сканируется специальными программами поиска ошибок в данных.

• Визуальная оценка используется в исключительных случаях, когда специализированным программным средствам не удалось повысить качество данных до приемлемого уровня. Чаще всего применяется при предобработке данных, загруженных в аналитическую систему: аналитик или эксперт с помощью имеющихся средств визуализации (таблиц, графиков и диаграмм) определяет наличие в данных аномальных значений, пропусков, шумов и т.д. Этот метод оценки качества применим только к небольшим объемам данных при решении отдельных нетиповых задач. Кроме того, визуально выявлять противоречия и дубликаты весьма проблематично.

  
  
  
  

МЕТОДЫ ОЦЕНКИ КАЧЕСТВА ДАННЫХ

1. Профайлинг — это процесс определения информации о некотором атрибуте (поле) источника данных и проверки ее соответствия заданным ограничениям.
    

Если параметры атрибута удовлетворяют этим ограничениям, то данные соответствуют определенному уровню качества, в противном случае необходимо принимать меры к приведению параметров к соответствующим ограничениям.

Профайлинг — достаточно развитый метод выявления проблем в данных. Однако он рассчитан на анализ «штатных», технических проблем, которые относятся к наиболее типичным и хорошо формализуемым. Поэтому профайлинг — это скорее метод предварительной оценки качества данных с целью выявления типичных проблем.

2. Визуальная оценка качества данных. Если объем данных не слишком велик, то для оценки их качества можно с успехом применять как встроенные средства визуализации аналитической платформы, так и сторонние программные средства, в которых можно просматривать данные в табличной и графической форме, рассчитывать статистические характеристики.
    
3. Оценка качества данных с помощью таблиц. Табличное представление позволяет делать общие выводы о наличии в данных структурных нарушений, пропусков, аномальных и фиктивных значений. Беглого взгляда на таблицу в большинстве случаев достаточно, чтобы оценить количество неполных строк и столбцов, пустых ячеек. Аномальные числовые значения могут резко выделяться на фоне окружающих данных количеством цифр. Даже простая сортировка по возрастанию позволит собрать предполагаемые аномальные значения в начале или конце столбца и легко обнаружить их.
    
4. Оценка качества данных с помощью графиков. Графики и диаграммы — эффективные средства для выявления проблем в данных. Например, с помощью графиков можно легко оценить наличие шумов в данных, а также пропущенных и аномальных значений. Если данные содержат шум, то он будет проявляться в сильной неравномерности значений ряда, прекрасно различаемой на графике. Аномальные значения будут отображаться в виде резких отклонений в ту или другую сторону. Наличие проблем в данных можно обнаружить с помощью гистограмм распределения числовых значений, например сумм продаж.
    

  

Фильтрация данных, обработка дубликатов и противоречий:

• Фильтрация:  Удаление нерелевантных данных, которые не соответствуют критериям анализа.

• Обработка дубликатов:  Удаление или объединение дублирующихся записей.

• Обработка противоречий:  Исправление или удаление данных, содержащих противоречивую информацию.

  

Выявление аномальных значений:

• Аномальные значения:  Выбросы или значения, значительно отличающиеся от остальных данных.

• Методы выявления: 

    * Визуальный анализ:  Просмотр данных на графиках для обнаружения выбросов.

    * Статистические методы:  Использование z-оценки, межквартильного размаха и других статистических методов.

    * Алгоритмы машинного обучения:  Использование алгоритмов для обнаружения аномалий.

Задача обработки аномальных значений состоит из двух этапов.

1. Обнаружение. 

2. Корректировка. 

• Удаление записи с аномальным значением. Если число записей в выборке данных существенно превышает минимум, требуемый для анализа, то записи, содержащие аномальные значения, можно просто удалить.

• Ручная замена аномальных значений. Применяется, если количество аномальных значений невелико и они могут быть обработаны вручную. При этом аналитик заменяет аномальные значения на другие, более соответствующие модели поведения данных.

• Сглаживание и фильтрация данных. Для обработки аномальных значений можно использовать методы частотной или пространственной фильтрации, применяемые при сглаживании данных и очистке от шумов (впрочем, это отдельный класс алгоритмов). При этом следует учитывать, что в результате обработки будут изменены не только аномальные значения, но и все значения ряда.

• Интерполяция данных. Аномальные значения заменяются другими, вычисленными на основе нескольких ближайших соседей.

• Замена на наиболее вероятное значение. Строится гистограмма распределения значений ряда, и по ней определяется значение, соответствующее моде гистограммы, которое и будет статистически наиболее вероятным.

  

Сокращение размерности данных (data reduction) — процесс сокращения объема исходного множества, загруженного для анализа в аналитическое приложение, таким образом, чтобы результирующее множество имело оптимальную размерность с точки зрения решаемой задачи и используемой модели.

Сокращение данных может производиться в двух режимах:

1. Режим отбора. Определяется значимость каждого признака исходного множества для решения конкретной задачи. Затем признаки отбираются в порядке уменьшения их значимости. Как только попадается признак, значимость которого меньше некоторого порога, отбор прекращается. Порог значимости устанавливается или на основе статистического анализа исходного множества, или опытным путем.

2. Режим исключения. Размер исходной выборки сокращается путем отбрасывания незначащих и избыточных данных..

9. # Технологии бизнес-аналитики: KDD и Data Mining. Методика извлечения знаний. Свойства знаний в аспекте бизнес-аналитики. Общие представления о задачах Data Mining.
    

KDD (Обнаружение знаний в базах данных) - это многоэтапный процесс, включающий в себя сбор данных, подготовку, преобразование, анализ, интерпретацию и оценку результатов.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdsJahFDijs7hlQqXiRIjYH2C8AiHpdbnO8SootZa_j11zEAi2djICNEYcsjDZsfIEC7oSz1smLxIPcJuICL3r8P-60h6ciQpZ4OGAV9tjefnNw0s8wWN4l14GHYw9Hc1-1sFWRSqU8IgF9xSMueO_oj0u_T1Oy_OmKlhJh?key=DNFeKhMTGi0oG-o6qf005Q)

• Получение данных: запросы, фильтры; эксперты помогают отобрать признаки, влияющие на результат.

Оптимальный источник – хранилище данных.

• Очистка данных: заполнение пропусков, подавление выбросов, сглаживание, исключение дубликатов и противоречий…

Задача очистки может иметь самостоятельную ценность.

• Преобразование данных: скользящее окно, вычисление агрегатов, приведение типов, выделение интервалов, квантование, сортировка, группировка, расчет производных столбцов…

  

Data Mining – обнаружение в «сырых» данных ранее неизвестных, нетривиальных, практически полезных и доступных интерпретации знаний, необходимых для принятия решений в различных сферах человеческой деятельности

  

Интерпретация — обнаруженные шаблоны представляются в виде решающих правил и их деревьев, структур кластеров, регрессии и т.д. На их основе формируются соответствующие знания. Обнаруженные знания могут быть использованы непосредственно, объединены со знаниями, полученными из других систем и предметных областей, применены для документирования и формирования отчетов. Также на данном этапе производится проверка наличия потенциальных конфликтов с ранее полученными знаниями и их разрешение.

  

Свойства знаний в аспекте бизнес-аналитики:

• Полезность: Знания должны быть актуальными, релевантными и полезными для решения конкретных бизнес-задач. 

• Точность: Знания должны быть  надежными и точными. 

• Достоверность:  Знания должны быть  подтверждены данными и  соответствовать  реальности. 

• Интерпретируемость:  Знания должны быть  легко  понимаемыми и  интерпретируемыми.

  

Общие представления о задачах Data Mining:

• Кластеризация:  Группировка данных в классы с  похожими  характеристиками.

• Классификация:  Предсказание  категории  для  новой  записи  на основе  данных  о  других  записях.

• Регрессия:  Предсказание  числового  значения  на  основе  данных  о  других  записях.

• Ассоциация:  Поиск  связей  между  данными,  например,  "клиенты, которые купили  товар A, также покупают  товар B."

• Анализ  аномалий:  Выявление  отклонений  от  нормального  поведения  данных.

• Предсказание:  Прогнозирование  будущих  значений  на  основе  исторических  данных.

  
  

10. # Модели и задачи Data Mining. Современное развитие классификации задач Data Mining. Основные свойства и характеристик методов Data Mining.
    

Data Mining – обнаружение в «сырых» данных ранее неизвестных, нетривиальных, практически полезных и доступных интерпретации знаний, необходимых для принятия решений в различных сферах человеческой деятельности

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXci3RvskvEuiUVeaborrGdbKb2b4gFNcQ_mSd2QQTHsiv_BQsCqDhtyce--ZXoKO01Ph-0ILewC_zn0lMMPCh-eq4rIWnVsTLP8lAcvQ4fjGZzQyAhn4cibMy_F-AhJZtJW_sPXF-2TG8VPINTAx8rrYLa_-8mbz_zmHikZ?key=DNFeKhMTGi0oG-o6qf005Q)

Описательная аналитика (Descriptive analytics) ближе к сложной визуализации и разведочному анализу – компактное описание множества объектов в виде правил, кластеров, шаблонов поведения, групп.

Описательные модели пытаются ответить:

• Какова структура клиентской базы?

• Какой профиль идеального клиента?

• Какие есть взаимосвязи между характеристиками клиентов?

• Какие события происходят одновременно?

  

Предсказательное моделирование (Predictive analytics)  отвечает на вопросы:

• Откликнется ли клиент на данную маркетинговую кампанию?

• Какой размер прибыли будет в следующем месяце?

• Какие из потенциальных клиентов вероятно совершат приобретение

• Услуги в следующем месяце?

• Какой прогнозируемый спрос на следующий период планирования?

Существует несколько условных классификаций задач Data Mining

1 Классификация – это установление зависимости дискретной выходной переменной от входных переменных.

2 Регрессия – это установление зависимости непрерывной выходной переменной от входных переменных.

3 Кластеризация – это группировка объектов (наблюдений, событий) на основе данных, описывающих свойства объектов. Объекты внутри кластера должны быть похожими друг на друга и отличаться от других, которые вошли в другие кластеры.

4 Ассоциация – выявление закономерностей между связанными событиями. 

  

Современное развитие классификации задач Data Mining:

• Развитие алгоритмов:  Постоянно появляются новые алгоритмы и методы Data Mining, более эффективные и точные.

• Обработка больших данных:  Развитие  технологий  Big Data  привело к  необходимости  разработки  новых  методов  Data Mining,  способных  обрабатывать  огромные  объемы  данных.

• Интеграция с другими областями:  Data Mining  интегрируется с  другими  областями,  такими  как  машинное  обучение,  глубокое  обучение,  компьютерное  зрение,  обработка  естественного  языка.

  

Основные свойства и характеристики методов Data Mining:

• Точность:  Насколько точно  метод  предсказывает  результаты. 

• Скорость:  Как быстро  метод  обрабатывает  данные. 

• Масштабируемость:  Способность  метода  работать  с  большими  объемами  данных.

• Интерпретируемость:  Насколько  легко  понять  результаты  метода. 

• Устойчивость:  Как  стабильно  работает  метод  при  изменении  данных.

  

11. # Ассоциативные правила. Назначение. Примеры использования. Базовые понятия. Методы поиска ассоциативных правил. Метрики ассоциативных правил. Интерпретация ассоциативных правил.
    

Ассоциация – выявление закономерностей между связанными событиями. Примером такой закономерности служит правило, указывающее, что из события X следует событие Y. Такие правила называются ассоциативными.

Пример: определение профиля посетителей веб-ресурса; выявление наборов товаров, которые в супермаркетах часто покупаются вместе или никогда не покупаются вместе.

Базовые понятия:

Предметный набор — это непустое множество объектов, появившихся в одной транзакции.

Транзакция — некоторое множество событий, происходящих совместно. 

Типичная транзакция —приобретение клиентом товара в супермаркете.

Методы поиска ассоциативных правил

•Алгоритм AIS. Первый алгоритм, предложенный Agrawal, Imielinski and Swami сотрудниками IBM Almaden в 1993 году. В алгоритме AIS кандидаты множества наборов генерируются и подсчитываются "на лету", во время сканирования базы данных.

•Алгоритм SETM. Создание этого алгоритма было мотивировано желанием использовать язык SQL для вычисления часто встречающихся наборов товаров. Формирует кандидатов "на лету", основываясь на преобразованиях базы данных.

•Алгоритм Apriori. Работа данного алгоритма состоит из нескольких этапов, каждый из этапов состоит из следующих шагов:

• формирование кандидатов;

• подсчет кандидатов.

Алгоритм использует одно из свойств поддержки, гласящее: поддержка любого набора объектов не может превышать минимальной поддержки любого из его подмножеств:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd6m4C5aCsUt1V_eyUr7RXDGol1cbMOYBnrSpz8RkeC3JSgRZv5WUL6ra5B61herDEMYUgQvCAsbmQfnd84XxsHa4GGCAHLH3QipOFtItUXK3P1XJX9INWIoCoq1EGOCHP-qVIMebqeUt7WOKdiKt_IudAuIYw7P84Gu_dywQ?key=DNFeKhMTGi0oG-o6qf005Q)

•Разновидности алгоритма Apriori, являющиеся его оптимизацией, предложены для сокращения количества сканирований базы данных, количества наборов-кандидатов или того и другого: AprioriTID и AprioriHybrid.

•Алгоритм DHP, также называемый алгоритмом хеширования.

•Алгоритм PARTITION - разбиения (разделения) заключается в сканировании транзакционной базы данных путем разделения ее на непересекающиеся разделы, каждый из которых может уместиться в оперативной памяти.

•Алгоритм DIC, Dynamic Itemset Counting разбивает базу данных на несколько блоков, каждый из которых отмечается так называемыми "начальными точками" (start point), и затем циклически сканирует базу данных.

Ассоциативные правила описывают связь между условием и следствием.

Эта связь характеризуется двумя показателями: поддержкой (support) и достоверностью (confidence).

  

Метрики ассоциативных правил:

Поддержка ассоциативного правила S—это отношение числа транзакций, которые содержат как условие, так и следствие к общему количеству транзакций.

Достоверность ассоциативного правила A → B — это мера точности правила.Определяется как отношение количества транзакций, содержащих и условие и следствие, к количеству транзакций, содержащих только условие

Лифт (оригинальное название — интерес) вычисляется следующим образом как отношение confidence к expected confidence, т.е. отношение достоверности правила, когда оба  элемента покупаются вместе к достоверности правила, когда один из элементов покупался (неважно, со вторым или без).

Левередж — это разность между наблюдаемой частотой, с которой условие и следствие появляются совместно (то есть поддержкой ассоциации), и произведением частот появления (поддержек) условия и следствия по отдельности.

Все множество ассоциативных правил можно разделить на три вида:

1.  Полезные правила – содержат информацию, которая ранее была неизвестна, но имеет логичное объяснение. Такие правила могут быть использованы для принятия решений.

2.  Тривиальные правила – содержат  информацию, которая уже известна. При анализе рыночных корзин в правилах с самой высокой поддержкой и достоверностью окажутся товары-лидеры продаж. Практическая ценность таких правил крайне низка.

3.  Непонятные правила – содержат информацию, которая не может быть объяснена. Это или аномальные значения, или глубоко скрытые знания. Необъяснимость правил может привести к непредсказуемым результатам, требуется дополнительный анализ

12. # Кластерный анализ. Формальное описание алгоритма кластеризации. Типы кластерных структур. Меры сходства. Методы определения расстояния между кластерами. Алгоритмы кластерного анализа.
    

 Кластерный анализ предназначен для разбиения данных на поддающиеся интерпретации группы , таким образом, чтобы элементы, входящие в одну группу были максимально «схожи» , а элементы из разных групп были максимально «отличными» друг от друга.

 Кластерный анализ – группа методов, используемых для классификации объектов или событий в относительно гомогенные (однородные) группы, которые называют кластерами (clusters).

 Цель кластеризации - поиск существующих структур.

  

В кластерном анализе–группируются строки, т.е. цель –анализ структуры множества объектов.

  

Кластерный анализ выполняет классификацию объектов.

  

Каждый объект (респондент) –точка в пространстве признаков.

  

Задача кластерного анализа –выделение «сгущений» точек, разбиение совокупности на однородные подмножества объектов (сегментация).

  

Типы кластерных структур

•Кластеры могут быть непересекающимися (non-overlapping), и пересекающимися (overlapping).

•Могут быть получены кластеры различной формы: длинными "цепочками", удлиненной формы, произвольной формы.

•Могут быть созданы кластеры определенных размеров (например, малых или крупных).

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTgM2kK6_ZNIONMElHQyjG0HbvDYB409Ltjw0xcBUpQBgEK35E0owSgIbP-xQIqS42Mmw7KeXfiShNMZWU7dNZUbKHv6sLL2lEcUy0rBtfMNuKG_LERojrSWbEvNoWwKNH5iao8EFHOUJFUgyLhl6dAlBerTxS3E37mk7nRg?key=DNFeKhMTGi0oG-o6qf005Q)![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeV9aaYg8FVtLMbixmczYlWSeFX2HAKHKcBgz90jvGADoysZgVl-K6GzCtr5_ifyowLSjIChc7wUT7y1f0p6owQpbNjllvDayGKpgzEL-PGN9plr85fgAweXHFv4OhMD8T7G6s-F12semvKslPND8wuY00OlWQLahbx8XnX?key=DNFeKhMTGi0oG-o6qf005Q)

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXecZm0e66aIkqtEPRVIHNZMlaJ-XbAXKlb_yTqlF4O6Z4NlYE4GvVCUN272HHJnXK41xWSQHDg0GzmyQ5y__p_Y6Ke-4muIb3o5-Ar6Q8iR8-AiF-2w44wdHOx92gOqOJmzG0tnb7hgtyCQ8vgouEw2RDn3nCIJgLNCJH6W?key=DNFeKhMTGi0oG-o6qf005Q)

  

Меры сходства:

• Евклидова дистанция:  Измеряет расстояние между объектами в многомерном пространстве.  расстояние(x,y) = {∑i (xi - yi)2 }1/2

• Манхэттенское расстояние:  Измеряет расстояние между объектами по осям. "хэмминговое" расстояние(x,y) =∑i |xi - yi|

• Косинусная мера:  Измеряет угол между векторами, представляющими объекты.

• Корреляция:  Измеряет степень линейной зависимости между объектами.

  

Расстояние между кластерами

•Метод ближнего соседа. Расстояние между двумя кластерами определяется расстоянием между двумя наиболее близкими объектами (ближайшими соседями) в различных кластерах.

•Метод наиболее удаленных соседей или полная связь. Рассчитывает расстояние между кластерами как максимальное расстояние между любыми двумя точками из разных кластеров.

•Метод Варда (Ward's method). В качестве расстояния между кластерами берется прирост суммы квадратов расстояний объектов до центров кластеров, получаемый в результате их объединения. Этот метод направлен на объединение близко расположенных кластеров и "стремится" создавать кластеры малого размера.

•Метод невзвешенного попарного арифметического среднего. В качестве расстояния между двумя кластерами берется среднее расстояние между всеми парами объектов в них.

  

Алгоритмы кластерного анализа:

• K-средних:  Алгоритм, который делит объекты на k кластеров, где k задается пользователем.

• Иерархическая кластеризация:  Алгоритм, который создает иерархическую структуру кластеров.

• Дендритное дерево:  Графическое представление иерархической структуры кластеров.

• DBSCAN:  Алгоритм, который кластеризует объекты, основываясь на плотности объектов в пространстве.

• Алгоритм EM:  Алгоритм, который использует вероятностную модель для кластеризации объектов.

  
  

13. # Кластерный анализ. Итеративные методы кластеризации. Проблемы методов. Стандартизация данных. Инструменты кластерного анализа. Кластерные силуэты как метод интерпретации и проверки согласованности в кластерах данных.
    

 Кластерный анализ предназначен для разбиения данных на поддающиеся интерпретации группы , таким образом, чтобы элементы, входящие в одну группу были максимально «схожи» , а элементы из разных групп были максимально «отличными» друг от друга.

 Кластерный анализ – группа методов, используемых для классификации объектов или событий в относительно гомогенные (однородные) группы, которые называют кластерами (clusters).

 Цель кластеризации - поиск существующих структур.

  

Итеративные методы кластеризации(метод k и g-средних, метод поиска сгущений) - это алгоритмы, которые работают по принципу итеративного уточнения кластеров. Они обычно начинаются с начальной точки (например, случайного выбора центроидов), а затем повторяют следующие шаги:

1. Присваивание объектов к кластерам: Каждый объект присваивается кластеру, к центроиду которого он ближе всего.

2. Обновление центроидов: Центроиды каждого кластера пересчитываются как средние значения всех объектов, принадлежащих этому кластеру.

Этот процесс повторяется до тех пор, пока не будет достигнуто сходимость, то есть пока кластеры перестанут меняться.

  

В отличие от иерархических методов, которые не требуют предварительных предположений относительно числа кластеров, для метода k-means необходимо иметь гипотезу о наиболее вероятном количестве кластеров/

  

G-means - популярный алгоритм кластеризации с автоматическим выбором числа кластеров.

  

В основе идеи EM-алгоритма лежит предположение, что любое наблюдение принадлежит ко всем кластерам, но с разной вероятностью. Формируются два дополнительных столбца: Номер кластера и Вероятность принадлежности.

Объект должен быть отнесен к тому кластеру, для которого вероятность принадлежности выше.

  
  
  

Проблемы методов кластеризации:

• Выбор метода: Существует множество различных методов кластеризации, и выбор подходящего зависит от конкретной задачи и типа данных.

• Выбор числа кластеров: Оптимальное число кластеров может быть неизвестно заранее, и выбор этого параметра может сильно повлиять на результаты кластеризации.

• Чувствительность к выбросам: Выбросы - это объекты, которые значительно отличаются от других объектов в наборе данных. Они могут исказить результаты кластеризации.

• Интерпретация результатов: После того, как кластеры найдены, важно уметь интерпретировать их и определить, что отличает один кластер от другого.

  

Стандартизация данных - это важный шаг в кластерном анализе, который позволяет избежать искажения результатов из-за различий в масштабе и единицах измерения переменных. 

  

Существуют различные методы стандартизации данных, например:

• Стандартизация z-оценки: Преобразование данных, чтобы среднее значение было равно 0, а стандартное отклонение - 1.

• Масштабирование: Преобразование данных в диапазон от 0 до 1.

  

Кластерные силуэты - это метод, который позволяет оценить качество кластеризации и проверить, насколько хорошо объекты соответствуют своим кластерам. Силуэт - это мера того, насколько хорошо объект соответствует своему кластеру по сравнению с другими кластерами. 

  

Значение силуэта колеблется от -1 до +1:

• Силуэт, близкий к +1, указывает на то, что объект хорошо соответствует своему кластеру.

• Силуэт, близкий к 0, указывает на то, что объект может быть на границе между двумя кластерами.

• Силуэт, близкий к -1, указывает на то, что объект может быть неправильно присвоен кластеру.

  

Анализ силуэтов может быть использован для:

• Оценки качества кластеризации: Чем выше среднее значение силуэта для всех объектов, тем лучше кластеризация.

• Определения оптимального числа кластеров: Можно попробовать разные числа кластеров и выбрать то, которое дает наивысшее значение силуэта.

• Идентификации выбросов: Объекты с низким значением силуэта могут быть выбросами.

  
  

14. # Классификация. Формальное описание задачи классификации. Представление результатов классификации. Методы построения правил классификации. Методы построения математических функций в задачах классификации.
    

Классификация — разбиение множества объектов или наблюдений на априорно заданные группы, называемые классами, внутри каждой из которых они предполагаются похожими друг на друга, имеющими примерно одинаковые свойства и признаки. При этом решение получается на основе анализа значений атрибутов (признаков). Классификация является одной из важнейших задач Data Mining. 

  

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc2YisWkp2DoqJ3MRmn3Udr202q2Fo8s1TowB5L-rAyvtZ5WZxlBY2JyaHDZzgwqa95pUj4J4qqfxT80jUS3Iocg4DlcBCK5fOQZ9aSKIkXkurp_Aym-kfXlkgMgr5ryYjqaV0hwoPtdR8boz91dvPnSZ-6b3ZAz-AloKAI?key=DNFeKhMTGi0oG-o6qf005Q)

Формально задачу классификации и регрессии можно описать следующим образом.

Имеется множество объектов:

I={i1, i2, …ij, …in },

где ij — исследуемый объект.

Каждый объект характеризуется набором переменных:

Ij = { x1, x2,..., xh,..., xm, y},

где xh — независимые переменные, значения которых известны и на основании которых определяется значение зависимой переменной y. 

В Data Mining часто набор независимых переменных обозначают в виде вектора:

X={x1, x2, …xj, …xn },

Каждая переменная xj может принимать значения из некоторого множества:

Ch={ch1, ch2, …},

Если значениями переменной являются элементы конечного множества, то

говорят, что она имеет категориальный тип.

Например, переменная наблюдение принимает значения на множестве значений {солнце, облачность, дождь}.

Если множество значений C={c1, c2, …cr, …ck } переменной y конечное, то задача называется задачей классификации.

Если переменная y принимает значение на множестве действительных чисел R , то задача называется задачей регрессии. 

Формальное описание задачи классификации:

• Входные данные: Набор объектов (примеров) с их характеристиками (признаками).

• Выходные данные: Модель классификации, которая позволяет предсказывать класс нового объекта на основе его признаков.

  

Представление результатов классификации:

В задачах классификации и регрессии обнаруженная функциональная зависимость между переменными может быть представлена одним из следующих способов:

- классификационные правила;
    
- деревья решений;
    
- математические функции.
    

  

1. Классификационные правила состоят из двух частей: условия и заключения:

если (условие) то (заключение).

Условием является проверка одной или нескольких независимых переменных с использованием  операций И, ИЛИ, НЕ.

Заключением является значение зависимой переменной или распределение ее вероятности по классам, например:

если (наблюдение = солнце И температура = жарко) то (игра = нет);

если (наблюдение = облачность И температура = холодно) то (игра = да).

2. Деревья решений — это способ представления правил в иерархической, последовательной структуре.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdIbN9bi7ksuVpQdm2DmwMFbn2Vy7ZBXRwDmrY-kp8cr8sUzkXqDLu33RXPpdia9coosMe_RF1CK9mchwVcmjkYP5SkEdTORj25V3EuJ2W8jRrLneaIADOe2RxkPoOla5UV-wEq6isGtFQaJb1s1ckicQjqtLty6h_t7MQ7TQ?key=DNFeKhMTGi0oG-o6qf005Q)

3. Математическая функция выражает отношение зависимой переменной от независимых переменных.

В этом случае анализируемые объекты рассматриваются как точки в (m + 1)-мерном пространстве.

Тогда переменные объекта ij = { x1, x2,..., xh,..., xm, y},

рассматриваются как координаты, а функция имеет следующий вид:

yj  ={ω0 +ωx1 +ωx2 +…. +ωxm}

где ω0, ω1, ωm — веса независимых переменных, в поиске которых и состоит задача нахождения классификационной функции. Очевидно, что все переменные должны быть представлены в виде числовых параметров.

Методы построения математических функций в задачах классификации:

• Линейные модели: Представляют зависимость класса от признаков линейным уравнением (например, логистическая регрессия).

• Нелинейные модели: Используют нелинейные функции для описания зависимости класса от признаков (например, нейронные сети).

• Ядерные методы: Преобразуют пространство признаков в более высокомерное пространство, позволяя строить более сложные нелинейные зависимости (например, опорные векторы).

  
  
  
  
  
  

15. # Задачи регрессионного анализа. Формальное описание концепции регрессионного анализа. Линейная регрессия. Метрики модели линейной регрессии. Интерпретация уравнения регрессии.
    

Регрессионный анализ – это набор статистических процедур для изучения зависимостей между случайными переменными (анализ взаимосвязей между зависимой переменной и одной или несколькими независимыми переменными). Основные задачи анализа:

• Прогнозирование: Предсказание значения зависимой переменной (целевой переменной) на основе значений независимых переменных (предикторов). Например, прогнозирование цены на недвижимость на основе ее площади, количества комнат и местоположения.

• Выявление зависимости: Определение характера связи между переменными, ее направления (прямая или обратная) и силы связи. Например, выявление зависимости между уровнем образования и заработной платы.

• Оценка влияния: Изучение влияния независимых переменных на зависимую переменную. Например, оценка влияния маркетинговых кампаний на продажи продукта.

  

Регрессионный анализ помогает понять, как «типичное» значение зависимой переменной изменяется при изменении одной из независимых переменных, в то время как другие независимые переменные остаются фиксированными.

  

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdQUxYZCu1xkITLEKFUnBVyAQTXOAwZZWLSWvcBph0T-5c-1DA0O7_qk-VcBpi1CSTqruc4Eqh6E2pChjLip8rmJUqT9w95W6vz71ftAfFiRbXcz0KNHs_vFrMUz2mndd2iwr0KCPyvNTGbb9lwtCp_by01OCOCKOzCix-WNQ?key=DNFeKhMTGi0oG-o6qf005Q)

Множественная линейная регрессия: Y = a1*X1 + a2*X2 + a3*X3 ……. an *Xn + b, где an — это коэффициенты, Xn — переменные и b — смещение.

В случае высокой коллинеарности переменных стандартная линейная и полиномиальная регрессии становятся неэффективными.

  

Метрики модели линейной регрессии:

• R-квадрат (R²):  Измеряет долю дисперсии зависимой переменной, объясняемой моделью. Чем ближе R² к 1, тем лучше модель описывает данные.

• Корень среднеквадратичной ошибки (RMSE): Измеряет среднюю ошибку модели. Чем меньше RMSE, тем точнее модель.

• Средняя абсолютная ошибка (MAE): Измеряет среднее абсолютное значение ошибки модели.

• Средняя квадратичная ошибка (MSE): Измеряет среднюю квадратичную ошибку модели.

• Adjusted R-squared: Подобен R-квадрат, но учитывает количество предикторов в модели.

  

Интерпретация уравнения регрессии 

• Когда предикторные переменные высоко коррелированы, сложно интерпретировать индивидуальные коэффициенты - Коррелированные предикторные переменные (correlated predictor variables) . 

• Когда предикторные переменные имеют линейную зависимость регрессия может быть нестабильной, либо ее невозможно вычислить - Мультиколлинеарность (multicollinearity). 

• Важный предиктор, который при его пропуске приводит к мнимым связям в уравнении регрессии - Искажающие переменные (confounding variables). 

• Связь между предикторной переменной и переменной исхода, которая не зависит от других переменных - Главные эффекты (main effects).

  

Интерпретация уравнения регрессии:

• Свободный член (b0):  Предсказывает значение Y, когда все независимые переменные равны нулю.

• Коэффициенты регрессии (b1, b2, ..., bn):  Показывает, как изменение независимой переменной на одну единицу влияет на значение зависимой переменной. 

Например, если коэффициент регрессии для площади жилья равен 1000, то увеличение площади на 1 квадратный метр приведет к увеличению цены на 1000 рублей (при прочих равных условиях).

  

16. # Задачи регрессионного анализа. Нелинейная регрессия. Роль факторных переменных в регрессионном анализе. Метрики модели нелинейной регрессии.
    

Регрессионный анализ – это мощный статистический инструмент, который позволяет выявить зависимость между одной зависимой переменной (откликом) и одной или несколькими независимыми переменными (предикторами). 

  

Основные задачи регрессионного анализа:

• Прогнозирование: предсказание значения зависимой переменной при заданных значениях независимых переменных. 

• Описание: выявление характера взаимосвязи между переменными и ее количественная оценка.

• Интерпретация: выявление и анализ влияния каждой независимой переменной на зависимую переменную.

• Контроль: управление зависимой переменной путем изменения независимых переменных.

  

Нелинейная регрессия:

Когда зависимость между переменными не может быть представлена прямой линией, мы говорим о нелинейной регрессии. 

  

Нелинейная регрессия - это статистический метод, используемый для моделирования нелинейных отношений между зависимой переменной и одной или несколькими независимыми переменными. Нелинейная регрессия используется, когда нам нужно подогнать нелинейную кривую к данным, где связь между независимыми и зависимыми переменными не является линейной.

В отличие от линейной регрессии, которая предполагает линейную связь между зависимой переменной и независимой(ыми) переменной(ыми), модели нелинейной регрессии могут иметь любую функциональную форму. Нелинейные регрессионные модели могут использоваться для оценки пар аметров сложных взаимосвязей, таких как экспоненциальные, логарифмические, квадратичные или тригонометрические функции.

  

Примеры нелинейных зависимостей:

• Экспоненциальная зависимость (рост населения)

• Логарифмическая зависимость (закон Вебера-Фехнера)

• Полиномиальная зависимость (зависимость траектории полета от времени)

  
  

Виды нелинейной регрессии:

1. Полиномиальная регрессия: 
    

• Моделирует нелинейно разделенные данные и сложные взаимосвязи. 

• Полный контроль над моделированием переменных объекта (выбор степени). 

• Необходимо обладать некоторыми знаниями о данных, для выбора наиболее подходящей степени. 

• При неправильном выборе степени модель может быть перенасыщена.

В полиномиальной регрессии степень некоторых независимых переменных превышает 1: Y = a1*X1 + (a2 )²*X2 + (a3 )⁴*X3 ……. an *Xn + b

2. Гребневая (ридж) регрессия - применяется для борьбы с избыточностью данных, когда независимые переменные коррелируют друг с другом, вследствие чего проявляется неустойчивость оценок коэффициентов многомерной линейной регрессии.
    

  
  

Роль факторных переменных в регрессионном анализе:

Факторные переменные – это категориальные переменные, которые не могут быть измерены количественно. Они принимают значения, относящиеся к различным категориям или группам.

Пример: 

•  Пол (мужчина/женщина)

•  Город проживания (Москва/Санкт-Петербург)

Включение факторных переменных в регрессионный анализ:

• Используются фиктивные переменные для представления категорий (0 или 1).

• Дают возможность исследовать влияние различных категорий на зависимую переменную.

  

Метрики модели нелинейной регрессии:

Основные метрики для оценки качества модели:

• Среднеквадратическая ошибка (MSE):  оценивает среднее расстояние между предсказанными и реальными значениями.

• Коэффициент детерминации (R-квадрат):  оценивает долю изменчивости зависимой переменной, объясненную моделью.

• Корень среднеквадратической ошибки (RMSE):  измеряет среднюю ошибку прогноза в тех же единицах измерения, что и зависимая переменная.

• Средняя абсолютная ошибка (MAE):  оценивает среднее абсолютное значение разницы между прогнозируемыми и фактическими значениями.

  
  
  
  

17. # Задачи регрессионного анализа. Модель логистической регрессии. Метрики логистической регрессии. Интерпретация результатов лОгит-модели.
    

Логистическая регрессия или логит-модель (англ. logit model) — статистическая модель, используемая для прогнозирования вероятности возникновения некоторого события путём его сравнения с логистической кривой. Эта регрессия выдаёт ответ в виде вероятности бинарного события (1 или 0). 

  

Модель логистической регрессии:

Логистическая регрессия — это особый вид регрессии, который используется для прогнозирования вероятности принадлежности объекта к определенному классу. 

  

Ключевые особенности:

• Дихотомическая зависимая переменная:  она принимает всего два значения, например, 0 или 1, "да" или "нет".

• Логистическая функция:  она преобразует линейную комбинацию предикторов в вероятность, заключенную между 0 и 1.

• Порог вероятности:  значение вероятности, которое отделяет два класса.

  

Пример: 

•  Прогнозирование вероятности покупки товара покупателем.

•  Определение вероятности наличия заболевания у пациента.

  

С помощью логистической регрессии можно оценивать вероятность того, что событие наступит для конкретного испытуемого (больной/здоровый, возврат кредита/дефолт и т.д.).

  

Метрики модели

1. Точность (accuracy). Процент (или доля) случаев, классифицированных правильно. 
    
2. Матрица несоответствий (confusion matrix). Отображение в табличной форме (2 × 2 в бинарном случае) количеств записей по их предсказанному и фактическому состояниям, или результату, классификации. 
    
3. Чувствительность (sensitivity). Процент (или доля) правильно классифицированных единиц. 
    
4. Специфичность (specificity). Процент (или доля) правильно классифицированных нулей. 
    
5. Прецизионность (precision). Процент (или доля) предсказанных единиц, которые фактически являются нулями. 
    
6. ROC-кривая (ROC curve). График чувствительности против специфичности. 
    
7. Лифт (lift). Метрический показатель, который измеряет степень эффективности модели.
    

  

Интерпретация результатов логит-модели:

• Коэффициенты модели:  показывают влияние каждого предиктора на вероятность принадлежности к положительному классу.

• Логистические коэффициенты:  интерпретируются как изменение шансов (odds) на принадлежность к положительному классу при изменении предиктора на единицу.

• p-value:  оценивает статистическую значимость коэффициента.

• Доверительные интервалы:  показывают диапазон значений, в котором находится истинный коэффициент с заданной вероятностью.

  
  

18. # Конструирование признаков. Биннинг и оптимальное квантование. Woe-  IVанализ. Особенности категориальных переменных в процедуре биннинга. Обработка пропусков (Null значений) в биннинге. Нелинейные зависимости и биннинг.
    

Конструирование признаков - это процесс создания новых признаков из существующих данных для улучшения качества модели машинного обучения. Это позволяет модели "увидеть" более сложные связи в данных и улучшить ее предсказательную способность.

  

Биннинг - это метод дискретизации непрерывных признаков, то есть разделение их на группы (бинны) с заданным количеством значений. 

  

Преимущества биннинга:

• Упрощение данных:  превращение непрерывного признака в категориальный.

• Сглаживание шума:  усреднение значений в пределах одного бинна.

• Улучшение интерпретируемости модели:  удобство анализа влияния категорий на зависимую переменную.

• Повышение устойчивости модели:  снижение влияния выбросов.

  

Оптимальное квантование - это процесс поиска оптимального числа бинов и границ бинов для максимально эффективного представления данных.

Используются различные методы, например:

• Метод равных интервалов: Разделение диапазона значений признака на равные интервалы.

• Метод равных частот:  Разделение диапазона значений признака на интервалы с равным количеством значений.

• Метод энтропии: Определение оптимального числа бинов, минимизируя энтропию данных.

  

Критерии оптимальности:

• Качество модели:  бины должны быть сформированы таким образом, чтобы модель максимально точно предсказывала зависимую переменную.

• Интерпретируемость:  бины должны быть логически осмысленными и удобными для анализа.

• Устойчивость:  бины должны быть устойчивы к изменениям в данных.

  

WoE (Weight of Evidence) - это мера силы связи между категориальным признаком и целевой переменной. Он показывает, насколько вероятно наблюдение принадлежит к положительному классу по сравнению с отрицательным классом для каждой категории.

  

IV (Information Value) - это мера информативности признака для предсказания целевой переменной.  Он показывает, насколько сильно признак влияет на предсказательную способность модели.

  

Особенности категориальных переменных в биннинге:

• Непрерывность:  категориальные переменные не являются непрерывными, поэтому их биннинг отличается от биннинга непрерывных признаков.

• Порядок:  бины для категориальных переменных могут быть упорядоченными (например, "низкий", "средний", "высокий") или неупорядоченными (например, "мужчина", "женщина").

• Количество категорий:  бины для категориальных переменных могут содержать любое количество категорий.

  

Обработка пропусков (Null значений) в биннинге:

• Удаление: Удаление строк с пропущенными значениями.

• Заполнение: Заполнение пропусков средним значением, модой или другими статистическими значениями.

• Создание отдельной категории:  Создание новой категории для пропущенных значений.

  

Нелинейные зависимости и биннинг:

Биннинг может помочь обнаружить и учесть нелинейные зависимости между признаками, например:

• S-образная зависимость:  Разделение непрерывного признака на бины может выявить нелинейную зависимость, которая не видна при линейной регрессии.

• Взаимодействие признаков:  Разделение двух признаков на бины может выявить взаимодействие между ними, которое не проявляется при индивидуальном рассмотрении.

  

Пример:

Представьте, что у вас есть данные о продажах автомобилей, где один из признаков - это "цена".  Вы можете разбить цену на бины, чтобы выявить, как продажи меняются в зависимости от ценового диапазона. Например, может оказаться, что продажи автомобилей в ценовом диапазоне от 10 000 до 20 000 долларов выше, чем в других диапазонах.

  
  
  

19. # Термин «модель данных» в аспекте целей и способов выполнения задач в технологиях Data Mining и Machine Learning. Виды моделей в Python. Этапы построения моделей машинного обучения (ML-моделей).
    

"Модель данных" в контексте Data Mining и Machine Learning  - это формальное описание данных, которое позволяет  анализировать, предсказывать и управлять информацией. Она описывает отношения между переменными, закономерности и зависимости, скрытые в данных.

  

Цели использования моделей данных:

• Понимание данных:  Визуализация связей и зависимостей, обнаружение закономерностей и аномалий.

• Прогнозирование: Предсказание будущих значений на основе имеющихся данных.

• Классификация:  Разделение данных на категории.

• Кластеризация: Группировка данных по сходству.

• Анализ тенденций: Выявление трендов и изменений в данных.

  

Способы выполнения задач:

• Data Mining:  Извлечение знаний из данных, поиск закономерностей и зависимостей.

• Machine Learning:  Обучение модели на данных для выполнения конкретных задач, таких как классификация, регрессия, кластеризация.

  

Python предлагает широкий набор библиотек для создания моделей машинного обучения, например, scikit-learn, TensorFlow, PyTorch. 

  

Основные виды моделей в Python:

• Регрессионные:  предсказывают непрерывные значения (например, цены на недвижимость, температура).

• Классификационные:  классифицируют объекты (например, спам/не спам, болезнь/здоровье).

• Кластерные:  группируют объекты по схожим признакам (например, сегментация клиентов).

• Деревья решений:  иерархические структуры для принятия решений (например, диагностика заболеваний).

• Нейронные сети:  сложные модели, имитирующие работу человеческого мозга (например, распознавание изображений).

• SVM (Support Vector Machines):  модели, которые находят оптимальную гиперплоскость для разделения классов (например, распознавание рукописного текста).

  

Этапы построения ML-моделей:

1. Сбор данных:  Сбор и подготовка данных для обучения модели.

2. Предварительная обработка данных:  Очистка, преобразование и стандартизация данных.

3. Выбор модели:  Выбор модели, подходящей для решаемой задачи.

4. Обучение модели:  Обучение модели на данных, минимизируя ошибку предсказания.

5. Оценка модели:  Оценка качества модели на тестовых данных.

6. Тюнинг параметров:  Настройка параметров модели для достижения оптимального качества.

7. Развертывание модели:  Размещение модели в реальной среде для использования.

  
  
  
  

20. # Использования языков программирования в low-code платформе для реализации продвинутой аналитики. Встраивание скриптов в поток обработки. Популярные библиотеки на Python.
    

Low-code платформы, благодаря своей простоте и интуитивности,  позволяют создавать приложения и аналитические решения без написания больших объемов кода. Однако, для реализации продвинутой аналитики и настройки индивидуальных алгоритмов, требуется возможность встраивания кода в процесс обработки данных.

Использование языков программирования в low-code:

• Расширение функциональности: Языки программирования предоставляют гибкость и контроль над алгоритмами, которые могут быть недоступны в стандартном функционале low-code платформы.

• Кастомизация: Встраивание кода позволяет создавать индивидуальные решения для обработки данных, настройки визуализации и реализации специфичных алгоритмов.

• Эффективность:  Использование языка программирования может повысить эффективность решения задачи, позволяя реализовать оптимизированные алгоритмы и использовать высокопроизводительные библиотеки.

  

Встраивание скриптов в поток обработки:

Low-code платформы предоставляют возможность встраивания скриптов в поток обработки данных. Это позволяет:

• Преобразовывать данные:  Применять пользовательские функции для изменения, фильтрации, агрегации и трансформации данных.

• Управлять логикой:  Реализовывать условия, циклы и другие логические операции для управления потоком обработки данных.

• Выполнять вычисления:  Выполнять сложные математические операции и статистический анализ.

  

Пример:

1. Графический интерфейс: Пользователь задает логику обработки данных в виде визуального потока (drag-and-drop).

2. Встраивание скрипта:  Внутри этого потока пользователь может добавить блок, в котором можно написать код на Python или другом языке.

3. Выполнение кода:  При запуске процесса платформа выполняет код скрипта,  результат которого передается дальше по потоку обработки.

  
  
  
  

Популярные библиотеки Python для аналитики:

• NumPy:  Библиотека для работы с массивами и матрицами, предоставляет функции для математических операций и линейной алгебры.

• Pandas:  Библиотека для анализа данных,  предоставляет инструменты для манипулирования, анализа и визуализации табличных данных.

• Scikit-learn:  Библиотека для машинного обучения, содержит множество алгоритмов для классификации, регрессии, кластеризации и др.

• Matplotlib: Библиотека для создания статических, интерактивных и анимированных графиков.

• Seaborn:  Библиотека для статистической визуализации, которая упрощает создание информативных и привлекательных графиков.

  

Пример использования Python в low-code:

Представьте, что вы используете low-code платформу для создания приложения для анализа продаж.  Вы хотите добавить функцию предсказания продаж на следующий месяц.  Встроенные инструменты low-code могут предоставлять только базовые алгоритмы предсказания.  

Чтобы создать более точную модель, вы можете встроить Python-скрипт, который использует библиотеку scikit-learn для обучения модели машинного обучения на исторических данных о продажах и прогнозирования продаж на следующий месяц.

  
  
**